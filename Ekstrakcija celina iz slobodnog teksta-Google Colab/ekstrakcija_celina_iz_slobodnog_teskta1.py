# -*- coding: utf-8 -*-
"""Ekstrakcija celina iz slobodnog teskta1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vQjTuvYIcTspWnQufxfL6YqrWXiyJFHO

# Ekstrakcija celina iz slobodnog teksta - Web Mining

**Predmet:** Web Mining
**Tema:** Ekstrakcija tematskih celina kombinovanjem NER, klasterovanja i web scraping-a
**KljuÄne tehnologije:** BERT, Sentence-BERT, KMeans, TF-IDF

# Instalacija biblioteka

Instaliramo sve potrebne biblioteke za kompletan Web Mining projekat.
"""

!pip install torch transformers sentence-transformers scikit-learn beautifulsoup4 requests feedparser matplotlib seaborn networkx wordcloud --quiet
print("âœ“ Sve biblioteke uspeÅ¡no instalirane!")

"""# Import biblioteka"""

# NLP i ML
from transformers import pipeline
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA

# Web scraping
import requests
from bs4 import BeautifulSoup
import feedparser

# Vizualizacija
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from wordcloud import WordCloud

# Ostalo
import numpy as np
from collections import defaultdict, Counter
import warnings
warnings.filterwarnings('ignore')

print("âœ“ Svi moduli importovani!")

"""# Korak 1: Prikupljanje teskta sa weba

Koristimo BBC RSS feed za prikupljanje realnih vesti.
"""

def scrape_bbc_rss(limit=20):
    """Prikuplja naslove i opise vesti sa BBC RSS feed-a"""
    url = "http://feeds.bbci.co.uk/news/rss.xml"

    try:
        feed = feedparser.parse(url)
        articles = []

        for entry in feed.entries[:limit]:
            title = entry.get('title', '')
            summary = entry.get('summary', '')
            text = f"{title}. {summary}"
            articles.append(text)

        print(f"âœ“ Prikupljeno {len(articles)} Älanaka sa BBC News")
        return articles
    except Exception as e:
        print(f"âœ— GreÅ¡ka pri scraping-u: {e}")
        print("  Koristim demo tekst...")
        # Fallback na demo tekst
        return [
            "Apple announced the new iPhone 15 in September with improved camera features",
            "Microsoft launched a new Azure AI service for enterprise customers",
            "Tesla is working on improving batteries for electric cars to extend range",
            "Elon Musk stated that AI will play a key role in future society",
            "Google is investing heavily in quantum computing research",
            "The iPhone 15 features USB-C port replacing the Lightning connector",
            "Azure AI offers natural language processing capabilities",
            "Electric vehicles are becoming more affordable for consumers",
            "Quantum computers could revolutionize cryptography and drug discovery",
            "Tim Cook presented the new Apple Watch during the keynote",
            "Microsoft CEO Satya Nadella emphasized cloud computing growth",
            "Tesla opened new Gigafactory in Texas for production",
            "OpenAI released GPT-4 with enhanced reasoning capabilities",
            "Google DeepMind announced breakthrough in protein folding",
            "Apple's market cap reached historic highs in 2024"
        ]

# Prikupi Älanke
articles = scrape_bbc_rss(limit=20)
text = ". ".join(articles)

print("\n" + "=" * 60)
print("PROJEKAT: ekstrakcija celina iz web sadrÅ¾aja")
print("=" * 60)
print(f"\nUkupna duÅ¾ina teksta: {len(text)} karaktera")
print(f"Broj Älanaka: {len(articles)}")

"""# Korak 2: Named Entity Recognition (NER)

Ekstrakcija entiteta: osobe (PER), organizacije (ORG), lokacije (LOC).
**Model:** BERT-large (340M parametra) - Deep Learning
"""

print("\n[KORAK 2] Ekstrakcija entiteta...")

ner_pipeline = pipeline(
    "ner",
    model="dbmdz/bert-large-cased-finetuned-conll03-english",
    aggregation_strategy="simple"
)

entities = ner_pipeline(text)
valid_types = ['PER', 'ORG', 'LOC', 'MISC']

entity_dict = defaultdict(list)
entity_scores = {}

for e in entities:
    if e['entity_group'] in valid_types:
        word = e['word'].replace("##", "").strip()
        entity_dict[e['entity_group']].append(word)
        entity_scores[word] = e['score']

print("\nğŸ“Š PREPOZNATI ENTITETI:")
print("-" * 60)
for etype, words in entity_dict.items():
    unique_words = list(set(words))[:5]  # Top 5
    print(f"{etype:8s}: {len(words):3d} ukupno | Primeri: {', '.join(unique_words)}")

"""# Korak 3: Priprema reÄenica"""

sentences = [s.strip() for s in text.split(".") if len(s.strip()) > 20]
print(f"\n[KORAK 3] Broj reÄenica: {len(sentences)}")
print(f"ProseÄna duÅ¾ina: {np.mean([len(s) for s in sentences]):.1f} karaktera")

"""# Korak 4: Sentence Embeddings

Konvertujemo teskt u numeriÄke vektore pomoÄ‡u transformer modela.
**Model:** Sentence-BERT (22M parametara) - Deep Learning
"""

print("\n[KORAK 4] Kreiranje embeddings-a...")
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(sentences, show_progress_bar=True)
print(f"âœ“ Shape: {embeddings.shape} (reÄenice x dimenzije)")

"""# Korak 5: Optimalan broj klastera

Koristimo Silhouette Score i Elbow metodu.
"""

print("\n[KORAK 5] TraÅ¾enje optimalnog broja klastera...")

silhouette_scores = []
inertias = []
K_range = range(2, min(8, len(sentences)))

for k in K_range:
    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels_temp = kmeans_temp.fit_predict(embeddings)

    score = silhouette_score(embeddings, labels_temp)
    silhouette_scores.append(score)
    inertias.append(kmeans_temp.inertia_)

    print(f"  K={k}: Silhouette={score:.4f}, Inertia={kmeans_temp.inertia_:.2f}")

optimal_k = K_range[np.argmax(silhouette_scores)]
print(f"\nâœ“ Optimalan broj klastera: {optimal_k}")

"""# Vizualizacija metrika za izbor K"""

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Silhouette Score
axes[0].plot(K_range, silhouette_scores, marker='o', linewidth=2, markersize=8)
axes[0].axvline(optimal_k, color='red', linestyle='--', label=f'Optimal K={optimal_k}')
axes[0].set_xlabel('Broj klastera (K)', fontsize=12)
axes[0].set_ylabel('Silhouette Score', fontsize=12)
axes[0].set_title('Silhouette Score po broju klastera', fontsize=14, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# Elbow Method
axes[1].plot(K_range, inertias, marker='s', linewidth=2, markersize=8, color='orange')
axes[1].axvline(optimal_k, color='red', linestyle='--', label=f'Optimal K={optimal_k}')
axes[1].set_xlabel('Broj klastera (K)', fontsize=12)
axes[1].set_ylabel('Inertia', fontsize=12)
axes[1].set_title('Elbow Method', fontsize=14, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

"""# Korak 6: Finalno klasterovanje"""

print("\n[KORAK 6] Finalno klasterovanje...")
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
labels = kmeans.fit_predict(embeddings)

clusters = defaultdict(list)
for sentence, label in zip(sentences, labels):
    clusters[label].append(sentence)

print(f"âœ“ ReÄenice podeljene u {optimal_k} tematske celine")
for i, sents in clusters.items():
    print(f"  Celina {i+1}: {len(sents)} reÄenica")

"""# Korak 7: Validacija unije klasteraa

MatematiÄka provera da li unija svih klastera daje kompletan originalni tekst.

Formalna definicija:
- Neka je T = originalni skup reÄenica
- C1, C2, ..., Ck = klasteri
- Validacija: U(C1 U C2 U ... U Ck) = T
"""

print("\n" + "="*70)
print("VALIDACIJA: UNIJA KLASTERA = ORIGINALNI TEKST")
print("="*70)

# 1. Izvuci sve reÄenice iz svih klastera
all_clustered_sentences = []
for cluster_id, sents in clusters.items():
    all_clustered_sentences.extend(sents)

# 2. Kreiraj skupove za poreÄ‘enje
original_set = set(sentences)
clustered_set = set(all_clustered_sentences)

print(f"\n[1] PROVERA KOMPLETNOSTI:")
print("-" * 70)
print(f"  Broj originalnih reÄenica:       {len(sentences)}")
print(f"  Broj reÄenica u klasterima:      {len(all_clustered_sentences)}")
print(f"  Broj jedinstvenih u originalu:   {len(original_set)}")
print(f"  Broj jedinstvenih u klasterima:  {len(clustered_set)}")

"""## MatematiÄka validacija

Proveravamo 4 kljuÄna uslova:
1. **Kompletnost**: Sve reÄenice su prisutne
2. **Bez duplikata**: Nijedna reÄenica se ne ponavlja
3. **Bez gubitaka**: Nijedna reÄenica nije izgubljena
4. **Disjunktnost**: Klasteri se ne preklapaju
"""

print(f"\n[2] MATEMATIÄŒKA VALIDACIJA:")
print("-" * 70)

# A. Provera: Unija = Original
is_union_valid = (original_set == clustered_set)
print(f"  âˆª(Câ‚, Câ‚‚, ..., C_{optimal_k}) = T: {'âœ“ DA' if is_union_valid else 'âœ— NE'}")

# B. Provera: Nema izgubljenih reÄenica
missing_sentences = original_set - clustered_set
print(f"  Izgubljene reÄenice: {len(missing_sentences)}")
if missing_sentences:
    print(f"    Primeri: {list(missing_sentences)[:2]}")

# C. Provera: Nema duplikata
duplicate_count = len(all_clustered_sentences) - len(clustered_set)
print(f"  Duplirane reÄenice: {duplicate_count}")

# D. Provera: Nema reÄenica van originala
extra_sentences = clustered_set - original_set
print(f"  ReÄenice van originala: {len(extra_sentences)}")

# E. Finalna validacija
if is_union_valid and duplicate_count == 0 and len(extra_sentences) == 0:
    print(f"\n  {'âœ“'*30}")
    print(f"  âœ“âœ“âœ“ VALIDACIJA USPEÅ NA âœ“âœ“âœ“")
    print(f"  {'âœ“'*30}")
else:
    print(f"\n  âœ—âœ—âœ— GREÅ KA U VALIDACIJI âœ—âœ—âœ—")

"""## Provera disjunktnosti klastera

Proveravamo da li se klasteri preklapaju (da li ista reÄenica pripada viÅ¡e klastera).
"""

print(f"\n[3] PROVERA DISJUNKTNOSTI KLASTERA:")
print("-" * 70)

cluster_sets = {cid: set(sents) for cid, sents in clusters.items()}
overlaps_found = False
overlap_details = []

for i in range(len(cluster_sets)):
    for j in range(i+1, len(cluster_sets)):
        overlap = cluster_sets[i] & cluster_sets[j]
        if overlap:
            overlaps_found = True
            overlap_details.append((i+1, j+1, len(overlap)))
            print(f"  âš  Preklapanje izmeÄ‘u Celine {i+1} i {j+1}: {len(overlap)} reÄenica")

if not overlaps_found:
    print(f"  âœ“ Klasteri su disjunktni (bez preklapanja)")
    print(f"  Cáµ¢ âˆ© Câ±¼ = âˆ… za sve i â‰  j")

"""## Formalna matematiÄka reprezentacija

Prikaz klasterovanja u matematiÄkoj notaciji.
"""

print(f"\n[4] FORMALNA MATEMATIÄŒKA NOTACIJA:")
print("-" * 70)
print(f"\n  Neka je T = originalni tekstualni korpus")
print(f"  T = {{sâ‚, sâ‚‚, ..., s_{len(sentences)}}} gde sáµ¢ = i-ta reÄenica")
print(f"\n  Klasterovanje deli T u k = {optimal_k} disjunktnih podskupova:")

for cid, sents in sorted(clusters.items()):
    percentage = (len(sents) / len(sentences)) * 100
    print(f"  C_{cid+1} = {{{len(sents)} reÄenica}} ({percentage:.1f}% od T)")

print(f"\n  MatematiÄka validacija:")
print(f"  âˆª(Câ‚ âˆª Câ‚‚ âˆª ... âˆª C_{optimal_k}) = T")
print(f"\n  Provera kardinalnosti:")
print(f"  |T| = {len(sentences)}")
print(f"  Î£|Cáµ¢| = {sum(len(sents) for sents in clusters.values())}")
print(f"  |T| = Î£|Cáµ¢| â†’ {'âœ“ TAÄŒNO' if len(sentences) == len(all_clustered_sentences) else 'âœ— NETAÄŒNO'}")

"""## Vizualizacija validacije

GrafiÄki prikaz distribucije i pokrivenosti reÄenica.
"""

print(f"\n[5] GRAFIÄŒKA VIZUALIZACIJA:")
print("-" * 70)

fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Grafik 1: Bar chart - Distribucija reÄenica
cluster_sizes = [len(sents) for sents in clusters.values()]
cluster_labels = [f'Celina {i+1}' for i in range(len(clusters))]
colors = plt.cm.Set3(range(len(clusters)))

axes[0].bar(cluster_labels, cluster_sizes, color=colors,
            edgecolor='black', linewidth=2, alpha=0.8)
axes[0].axhline(y=len(sentences), color='red', linestyle='--', linewidth=2,
                label=f'Ukupno: {len(sentences)} reÄenica')
axes[0].set_ylabel('Broj reÄenica', fontsize=13, fontweight='bold')
axes[0].set_xlabel('Tematske celine', fontsize=13, fontweight='bold')
axes[0].set_title('Distribucija reÄenica po celinama', fontsize=15, fontweight='bold')
axes[0].legend(fontsize=11)
axes[0].grid(axis='y', alpha=0.3, linestyle='--')

# Dodaj brojeve na barovima
for i, (label, size) in enumerate(zip(cluster_labels, cluster_sizes)):
    axes[0].text(i, size + 0.3, str(size), ha='center', fontsize=12, fontweight='bold')

# Grafik 2: Pie chart - Procenat pokrivanja
percentages = [(size/len(sentences))*100 for size in cluster_sizes]
explode = [0.05] * len(clusters)  # "Izvuci" sve delove malo

wedges, texts, autotexts = axes[1].pie(
    cluster_sizes,
    labels=cluster_labels,
    autopct='%1.1f%%',
    colors=colors,
    startangle=90,
    explode=explode,
    wedgeprops={'edgecolor': 'black', 'linewidth': 2},
    textprops={'fontsize': 11, 'fontweight': 'bold'}
)

# PoveÄ‡aj font za procente
for autotext in autotexts:
    autotext.set_color('white')
    autotext.set_fontsize(12)
    autotext.set_fontweight('bold')


axes[1].set_title('Procentualna zastupljenost celina', fontsize=15, fontweight='bold')


plt.tight_layout()
plt.show()

print(f"âœ“ Vizualizacija kreirana")
print(f"  Zbir svih procenata = {sum(percentages):.1f}% (oÄekivano: 100%)")

"""## Rekonstrukcija originalnog teksta

Dokazujemo da moÅ¾emo potpuno rekonstruisati originalni tekst iz klastera.

"""

print(f"\n[6] REKONSTRUKCIJA ORIGINALNOG TEKSTA:")
print("-" * 70)

# RekonstruiÅ¡i tekst spajanjem svih klastera
reconstructed_sentences = []
for cluster_id in sorted(clusters.keys()):
    reconstructed_sentences.extend(clusters[cluster_id])

# Kreiraj tekstove za poreÄ‘enje
original_text_full = ". ".join(sentences)
reconstructed_text_full = ". ".join(reconstructed_sentences)

# Provera identiÄnosti sadrÅ¾aja (ignoriÅ¡i redosled)
content_identical = set(sentences) == set(reconstructed_sentences)
count_identical = len(sentences) == len(reconstructed_sentences)

print(f"  ğŸ“„ Originalni tekst:")
print(f"     - Broj reÄenica: {len(sentences)}")
print(f"     - Ukupna duÅ¾ina: {len(original_text_full)} karaktera")
print(f"     - Primer: \"{sentences[0][:60]}...\"")

print(f"\n  ğŸ”„ Rekonstruisani tekst (iz klastera):")
print(f"     - Broj reÄenica: {len(reconstructed_sentences)}")
print(f"     - Ukupna duÅ¾ina: {len(reconstructed_text_full)} karaktera")
print(f"     - Primer: \"{reconstructed_sentences[0][:60]}...\"")

print(f"\n  âœ“ Provera identiÄnosti:")
print(f"     - Broj reÄenica identiÄan: {'âœ“ DA' if count_identical else 'âœ— NE'}")
print(f"     - SadrÅ¾aj identiÄan: {'âœ“ DA' if content_identical else 'âœ— NE'}")

if content_identical and count_identical:
    print(f"\n  {'='*70}")
    print(f"  ğŸ¯ USPEÅ NA REKONSTRUKCIJA!")
    print(f"  Unija svih klastera = Kompletan originalni tekst")
    print(f"  {'='*70}")

"""## Validacioni izveÅ¡taj

Formalni izveÅ¡taj o validaciji za seminarski rad.
"""

print(f"\n[7] GENERISANJE VALIDACIONOG IZVEÅ TAJA:")
print("="*70)

validation_report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           VALIDACIONI IZVEÅ TAJ - Unija klastera                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. OSNOVNI PODACI
   â”œâ”€ Ukupan broj reÄenica (|T|): {len(sentences)}
   â”œâ”€ Broj klastera (k): {optimal_k}
   â”œâ”€ Broj reÄenica u klasterima: {len(all_clustered_sentences)}
   â””â”€ Datum analize: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M')}

2. MATEMATIÄŒKA VALIDACIJA
   â”œâ”€ |T| = |âˆªCáµ¢|: {'âœ“ VALIDNO' if len(sentences) == len(all_clustered_sentences) else 'âœ— GREÅ KA'}
   â”œâ”€ Nema izgubljenih reÄenica: {'âœ“ DA' if len(missing_sentences) == 0 else f'âœ— NE ({len(missing_sentences)})'}
   â”œâ”€ Nema duplikata: {'âœ“ DA' if duplicate_count == 0 else f'âœ— NE ({duplicate_count})'}
   â”œâ”€ Nema ekstra reÄenica: {'âœ“ DA' if len(extra_sentences) == 0 else f'âœ— NE ({len(extra_sentences)})'}
   â””â”€ Klasteri disjunktni: {'âœ“ DA' if not overlaps_found else 'âœ— NE'}

3. DISTRIBUCIJA REÄŒENICA PO CELINAMA
"""

for cid, sents in sorted(clusters.items()):
    percentage = (len(sents) / len(sentences)) * 100
    bar = 'â–ˆ' * int(percentage / 5)  # Vizuelni bar
    validation_report += f"   â”œâ”€ Celina {cid+1}: {len(sents):2d} reÄenica ({percentage:5.1f}%) {bar}\n"

validation_report += f"""
4. FORMALNA MATEMATIÄŒKA NOTACIJA
   âˆª(Câ‚ âˆª Câ‚‚ âˆª ... âˆª C_{optimal_k}) = T

   Provera: Î£|Cáµ¢| = {' + '.join(str(len(c)) for c in clusters.values())} = {sum(len(c) for c in clusters.values())} = |T|

5. ZAKLJUÄŒAK
   {'âœ“'*35}
   VALIDACIJA USPEÅ NA - Unija klastera daje kompletan
   originalni tekst bez gubitka ili duplikacije podataka.
   {'âœ“'*35}
"""

print(validation_report)

# SaÄuvaj izveÅ¡taj
validation_success = (
    is_union_valid and
    duplicate_count == 0 and
    len(missing_sentences) == 0 and
    len(extra_sentences) == 0 and
    not overlaps_found
)

print(f"\n{'='*70}")
print(f"FINALNI STATUS: {'SVE PROVERE PROÅ LE' if validation_success else 'POSTOJE GREÅ KE'}")
print(f"{'='*70}")

"""# Korak 8: TF-IDF analiza kljuÄnih reÄi

Indentifikujemo najvaÅ¾nije reÄi za svaku tematsku celinu.
"""

print("\n[KORAK 8] TF-IDF analiza kljuÄnih reÄi...")

def get_top_keywords(cluster_sentences, top_n=5):
    """Ekstraktuje top N kljuÄnih reÄi pomoÄ‡u TF-IDF"""
    text_combined = " ".join(cluster_sentences)

    vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
    try:
        tfidf_matrix = vectorizer.fit_transform([text_combined])
        feature_names = vectorizer.get_feature_names_out()
        scores = tfidf_matrix.toarray()[0]

        top_indices = scores.argsort()[-top_n:][::-1]
        keywords = [feature_names[i] for i in top_indices if scores[i] > 0]
        return keywords
    except:
        return []

cluster_keywords = {}
for cluster_id, sents in clusters.items():
    keywords = get_top_keywords(sents, top_n=5)
    cluster_keywords[cluster_id] = keywords
    print(f"\nCelina {cluster_id + 1} - KljuÄne reÄi: {', '.join(keywords)}")

"""# Korak 9: Detaljni prikaz celina sa entitetima"""

print("\n" + "=" * 70)
print("REZULTATI: TEMATSKE CELINE SA ANALIZOM")
print("=" * 70)

for cluster_id, sents in sorted(clusters.items()):
    print(f"\n{'='*70}")
    print(f"[CELINA {cluster_id + 1}] - {len(sents)} reÄenica")
    print(f"{'='*70}")

    # KljuÄne reÄi
    if cluster_id in cluster_keywords and cluster_keywords[cluster_id]:
        print(f"\nğŸ”‘ KljuÄne reÄi: {', '.join(cluster_keywords[cluster_id])}")

    # Entiteti u celini
    cluster_text = " ".join(sents)
    cluster_entities = ner_pipeline(cluster_text)

    cluster_orgs = []
    cluster_persons = []
    cluster_locs = []

    for e in cluster_entities:
        word = e['word'].replace("##", "").strip()
        if e['entity_group'] == 'ORG':
            cluster_orgs.append(word)
        elif e['entity_group'] == 'PER':
            cluster_persons.append(word)
        elif e['entity_group'] == 'LOC':
            cluster_locs.append(word)

    if cluster_orgs:
        print(f"ğŸ¢ Organizacije: {', '.join(set(cluster_orgs))}")
    if cluster_persons:
        print(f"ğŸ‘¤ Osobe: {', '.join(set(cluster_persons))}")
    if cluster_locs:
        print(f"ğŸ“ Lokacije: {', '.join(set(cluster_locs))}")

    # ReÄenice
    print(f"\nğŸ“ ReÄenice:")
    for i, s in enumerate(sents[:3], 1):  # PrikaÅ¾i prvih 3
        print(f"  {i}. {s[:100]}{'...' if len(s) > 100 else ''}")

    if len(sents) > 3:
        print(f"  ... i joÅ¡ {len(sents) - 3} reÄenica")

"""# Korak 10: Vizualizacija klastera u 2D prostoru

Koristimo PCA za redukciju dimenzija i prikaz klastera.
"""

print("\n[KORAK 10] Vizualizacija klastera...")

# PCA redukcija na 2D
pca = PCA(n_components=2, random_state=42)
embeddings_2d = pca.fit_transform(embeddings)

# Plot
plt.figure(figsize=(12, 8))
colors = plt.cm.Set3(np.linspace(0, 1, optimal_k))

for i in range(optimal_k):
    mask = labels == i
    plt.scatter(
        embeddings_2d[mask, 0],
        embeddings_2d[mask, 1],
        c=[colors[i]],
        label=f'Celina {i+1} ({sum(mask)} rek.)',
        s=100,
        alpha=0.7,
        edgecolors='black',
        linewidth=0.5
    )

# Centri klastera
centers_2d = pca.transform(kmeans.cluster_centers_)
plt.scatter(
    centers_2d[:, 0],
    centers_2d[:, 1],
    c='red',
    marker='X',
    s=300,
    edgecolors='black',
    linewidth=2,
    label='Centri',
    zorder=5
)

plt.xlabel('PCA Komponenta 1', fontsize=12)
plt.ylabel('PCA Komponenta 2', fontsize=12)
plt.title('Vizualizacija tematskih celina (PCA projekcija)', fontsize=14, fontweight='bold')
plt.legend(loc='best', fontsize=10)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

explained_var = pca.explained_variance_ratio_
print(f"\nâœ“ PCA objaÅ¡njava {sum(explained_var)*100:.1f}% varijanse")

"""# Korak 11: Graf povezanosti entiteta

Prikazujemo kako su razliÄiti entiteti povezani u tekstu.
"""

print("\n[KORAK 11] Kreiranje grafa entiteta...")

# Kreiranje grafa
G = nx.Graph()

# Dodaj sve organizacije kao Ävorove
all_orgs = [e['word'].replace("##", "").strip()
            for e in entities if e['entity_group'] == 'ORG']
org_counts = Counter(all_orgs)

# Dodaj top organizacije
top_orgs = [org for org, count in org_counts.most_common(10)]

for org in top_orgs:
    G.add_node(org, size=org_counts[org] * 100)

# Dodaj veze (organizacije koje se pojavljuju u istoj reÄenici)
for sentence in sentences:
    sentence_orgs = [org for org in top_orgs if org.lower() in sentence.lower()]

    for i, org1 in enumerate(sentence_orgs):
        for org2 in sentence_orgs[i+1:]:
            if G.has_edge(org1, org2):
                G[org1][org2]['weight'] += 1
            else:
                G.add_edge(org1, org2, weight=1)

# Crtanje grafa
plt.figure(figsize=(14, 10))
pos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)

# VeliÄine Ävorova
node_sizes = [G.nodes[node].get('size', 300) for node in G.nodes()]

# Debljina ivica
edges = G.edges()
weights = [G[u][v]['weight'] for u, v in edges]
max_weight = max(weights) if weights else 1
widths = [3 * w / max_weight for w in weights]

# Crtanje
nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='lightblue',
                       edgecolors='black', linewidths=2, alpha=0.9)
nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')
nx.draw_networkx_edges(G, pos, width=widths, alpha=0.6, edge_color='gray')

plt.title('Graf povezanosti organizacija u tekstu', fontsize=14, fontweight='bold', pad=20)
plt.axis('off')
plt.tight_layout()
plt.show()

print(f"\nâœ“ Graf sadrÅ¾i {G.number_of_nodes()} Ävorova i {G.number_of_edges()} veza")

"""# Korak 12: Word Clouds za svaku celinu

Vizualizacija najÄeÅ¡Ä‡ih reÄi u svakoj tematskoj celini.
"""

print("\n[KORAK 12] Generisanje Word Clouds...")

fig, axes = plt.subplots(1, optimal_k, figsize=(6*optimal_k, 5))
if optimal_k == 1:
    axes = [axes]

for cluster_id, ax in enumerate(axes):
    cluster_text = " ".join(clusters[cluster_id])

    try:
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color='white',
            colormap='viridis',
            max_words=50,
            stopwords={'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for'}
        ).generate(cluster_text)

        ax.imshow(wordcloud, interpolation='bilinear')
        ax.set_title(f'Celina {cluster_id + 1}', fontsize=14, fontweight='bold')
        ax.axis('off')
    except:
        ax.text(0.5, 0.5, 'Nedovoljno teksta', ha='center', va='center')
        ax.axis('off')

plt.tight_layout()
plt.show()

"""# Korak 13: Finalna evaluacija i statistika"""

final_score = silhouette_score(embeddings, labels)

print("\n" + "=" * 70)
print("FINALNA EVALUACIJA PROJEKTA")
print("=" * 70)

print(f"\nğŸ“Š STATISTIKA:")
print(f"  â€¢ Ukupno reÄenica: {len(sentences)}")
print(f"  â€¢ Broj tematskih celina: {optimal_k}")
print(f"  â€¢ ProseÄna veliÄina celine: {len(sentences)/optimal_k:.1f} reÄenica")
print(f"  â€¢ Ukupno entiteta: {sum(len(v) for v in entity_dict.values())}")
print(f"    - Organizacije: {len(entity_dict.get('ORG', []))}")
print(f"    - Osobe: {len(entity_dict.get('PER', []))}")
print(f"    - Lokacije: {len(entity_dict.get('LOC', []))}")

print(f"\nğŸ¯ KVALITET GRUPISANJA:")
print(f"  â€¢ Silhouette Score: {final_score:.4f}")

if final_score > 0.5:
    quality = "ODLIÄŒNO âœ“"
elif final_score > 0.3:
    quality = "DOBRO âœ“"
elif final_score > 0.1:
    quality = "UMERENO âš "
else:
    quality = "LOÅ E âœ—"

print(f"  â€¢ Ocena: {quality}")
print(f"  â€¢ PCA varijansa: {sum(pca.explained_variance_ratio_)*100:.1f}%")

print(f"\nğŸ” VALIDACIJA UNIJE:")
print(f"  â€¢ Unija klastera = Original: {'âœ“ DA' if validation_success else 'âœ— NE'}")
print(f"  â€¢ Bez duplikata: {'âœ“ DA' if duplicate_count == 0 else 'âœ— NE'}")
print(f"  â€¢ Bez gubitaka: {'âœ“ DA' if len(missing_sentences) == 0 else 'âœ— NE'}")
print(f"  â€¢ Disjunktni klasteri: {'âœ“ DA' if not overlaps_found else 'âœ— NE'}")

print("\n" + "=" * 70)
print("âœ… PROJEKAT ZAVRÅ EN!")
print("=" * 70)

"""# ZakljuÄak

## Å ta projekat demonstrira:

### 1. **Web Mining komponenta**
- Automatsko prikupljanje podataka sa weba (BBC RSS feed)
- Parsing i ÄiÅ¡Ä‡enje tekstualnih podataka
- Obrada realnih korpusa vesti

### 2. **Deep Learning obrada**
- **BERT-large NER** (340M parametara) - Named Entity Recognition
- **Sentence-BERT** (22M parametara) - Semantic embeddings
- Transformer arhitektura sa self-attention mehanizmom

### 3. **Machine Learning klasterovanje**
- KMeans algoritam za temetsko grupisanje
- Automatsko odreÄ‘ivanje optimalnog broja klastera
-Silhouette Score i Elbow metoda za evaluaciju

### 4. **Analiza sadrÅ¾aja**
- TF-IDF ekstrakcija kl **bold text**juÄnih reÄi
-Indentifikacija dominantnih entiteta po celini
- Mapiranje organizacija, osoba i lokacija

### 5. **MatematiÄka validacija**
- Provera unije klastera: U(C1 U C2 ... U Ck) = T
- Dokazivanje kompletnosti i disjunktnosti
- Rekonstrukcija originalnog teksta

### 6. **Vizualizacija**
- PCA projekcija klastera u 2D prostoru
- Graf povezanosti entiteta (NetworkX)
- Word clouds po tematskim celinama
- Distribucione vizualizacije

---

## Programski jezik:
- Python

## Tehnologije:

**NLP & Deep Learning:**
- transformers (Hugging Face)
- sentence-transformers
- BERT, Sentence-BERT

**Machine Learning:**
- scikit-learn (KMeans, PCA, TF-IDF)
- numpz, scipy

**Web Mining:**
- BeautifulSoup4
- feedparser
- requests

**Vizualizacija**
- matplotlib, seaborn
- NetworkX
- WordCloud

---

## Rezultati:

- **UspeÅ¡no prikupljeno** {len(articles)} Älanaka
- **Ekstraktovano** {sum(len(v) for v in entity_dict.values())} entiteta
- **Kreirano** {optimal_k} tematske celine
- **Silhouette Score:** {final_score:.3f}
- **Validacija unije:** {'USPEÅ NA' if validation_success else 'GREÅ KA'}

---

**Projekat u potpunosti pokriva oblast Web Mining-a kombinujuÄ‡i:**
- Prikupljanje podataka sa weba
- Deep learning obradu teksta
- Tematsko grupisamke i analizu
- MatematiÄku validaciju rezultata
- Vizualizaciju i interpretaciju

---

**Hvala na paÅ¾nji**








"""